# PA3 Interpretations

## Interpretations after Training

Answer the following questions after completing your training steps, before analyzing the hyperparameters in a Jupyter notebook.

### How well did each grid search appear to work based on the *training* data's classification report?

### How does your interpretation change (or not change) after seeing the test data's classification report? Why do you think the results are what they are?

## Interpretation after Analysis

Your Jupyter notebook should have markdown with analysis after each output or graph created to analyze the cv results. Below answer the big picture questions related to your analysis.

### Did the effect of hyperparameter values match between metrics, or differ? In what way?


### What about the analysis graphs surprised you? If nothing surprised you, what about the graphs were as expected?


### Did either or both searches result in models with overfitting? How can you tell?


### Did the best model perform as well on the test data as you expected? How so?


### This data was imbalanced. What did we do because of that in our process? Was it effective?


### What else could we try later to deal with the imbalance?

